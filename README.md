# image
I used the activation function as x^2/(1+e^-x)
###### The Graph of the function has been attached
I tried using other types of activation functions as well but this seemed to work better than the rest
I used this particular activation function because it would be good to be used in classification problem and it is pretty easy to put in use and easy to compute.
This function has less gradient squishing property
The inspiration of using sigmoid as a part of it comes from the varied use of sigmoid in self attention LSTMs
